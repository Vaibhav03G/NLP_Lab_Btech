{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 29 tokens\n",
      "\n",
      "{'Gensim': 0, 'Python': 1, 'a': 2, 'algorithms.': 3, 'and': 4, 'as': 5, 'designed': 6, 'digital': 7, 'documents': 8, 'efficiently': 9, 'for': 10, 'free': 11, 'is': 12, 'learning': 13, 'library': 14, 'machine': 15, 'open-source': 16, 'painlessly': 17, 'possible.': 18, 'process': 19, 'raw,': 20, 'representing': 21, 'semantic': 22, 'texts': 23, 'to': 24, 'unstructured': 25, 'unsupervised': 26, 'using': 27, 'vectors,': 28}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import gensim\n",
    "from gensim import corpora\n",
    "\n",
    "text1 = [\"\"\"Gensim is a free open-source Python library for representing documents as semantic vectors,\n",
    "           as efficiently and painlessly as possible. Gensim is designed \n",
    "           to process raw, unstructured digital texts using unsupervised machine learning algorithms.\"\"\"]\n",
    "\n",
    "tokens1 = [[item for item in line.split()] for line in text1]\n",
    "g_dict1 = corpora.Dictionary(tokens1)\n",
    "\n",
    "print(\"The dictionary has: \" +str(len(g_dict1)) + \" tokens\\n\")\n",
    "print(g_dict1.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 57 tokens\n",
      "\n",
      "{'analyzing': 0, 'and': 1, 'branch': 2, 'consists': 3, 'data': 4, 'deriving': 5, 'efficient': 6, 'for': 7, 'from': 8, 'in': 9, 'information': 10, 'is': 11, 'manner': 12, 'nlp': 13, 'of': 14, 'processes': 15, 'science': 16, 'smart': 17, 'systematic': 18, 'text': 19, 'that': 20, 'the': 21, 'understanding': 22, 'analysis': 23, 'as': 24, 'automated': 25, 'automatic': 26, 'by': 27, 'can': 28, 'chunks': 29, 'components': 30, 'entity': 31, 'etc': 32, 'extraction': 33, 'its': 34, 'machine': 35, 'massive': 36, 'named': 37, 'numerous': 38, 'one': 39, 'organize': 40, 'perform': 41, 'problems': 42, 'range': 43, 'recognition': 44, 'relationship': 45, 'segmentation': 46, 'sentiment': 47, 'solve': 48, 'speech': 49, 'such': 50, 'summarization': 51, 'tasks': 52, 'topic': 53, 'translation': 54, 'utilizing': 55, 'wide': 56}\n"
     ]
    }
   ],
   "source": [
    "from gensim.utils import simple_preprocess\n",
    "from gensim import corpora\n",
    "\n",
    "text2 = open('sample_text.txt', encoding ='utf-8')\n",
    " \n",
    "tokens2 =[]\n",
    "for line in text2.read().split('.'):\n",
    "  tokens2.append(simple_preprocess(line, deacc = True))\n",
    "\n",
    "g_dict2 = corpora.Dictionary(tokens2)\n",
    "\n",
    "print(\"The dictionary has: \" +str(len(g_dict2)) + \" tokens\\n\")\n",
    "print(g_dict2.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The dictionary has: 81 tokens\n",
      "\n",
      "{'Gensim': 0, 'Python': 1, 'a': 2, 'algorithms.': 3, 'and': 4, 'as': 5, 'designed': 6, 'digital': 7, 'documents': 8, 'efficiently': 9, 'for': 10, 'free': 11, 'is': 12, 'learning': 13, 'library': 14, 'machine': 15, 'open-source': 16, 'painlessly': 17, 'possible.': 18, 'process': 19, 'raw,': 20, 'representing': 21, 'semantic': 22, 'texts': 23, 'to': 24, 'unstructured': 25, 'unsupervised': 26, 'using': 27, 'vectors,': 28, 'analyzing': 29, 'branch': 30, 'consists': 31, 'data': 32, 'deriving': 33, 'efficient': 34, 'from': 35, 'in': 36, 'information': 37, 'manner': 38, 'nlp': 39, 'of': 40, 'processes': 41, 'science': 42, 'smart': 43, 'systematic': 44, 'text': 45, 'that': 46, 'the': 47, 'understanding': 48, 'analysis': 49, 'automated': 50, 'automatic': 51, 'by': 52, 'can': 53, 'chunks': 54, 'components': 55, 'entity': 56, 'etc': 57, 'extraction': 58, 'its': 59, 'massive': 60, 'named': 61, 'numerous': 62, 'one': 63, 'organize': 64, 'perform': 65, 'problems': 66, 'range': 67, 'recognition': 68, 'relationship': 69, 'segmentation': 70, 'sentiment': 71, 'solve': 72, 'speech': 73, 'such': 74, 'summarization': 75, 'tasks': 76, 'topic': 77, 'translation': 78, 'utilizing': 79, 'wide': 80}\n"
     ]
    }
   ],
   "source": [
    "g_dict1.add_documents(tokens2)\n",
    "\n",
    "print(\"The dictionary has: \" +str(len(g_dict1)) + \" tokens\\n\")\n",
    "print(g_dict1.token2id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words :  [[(0, 2), (1, 1), (2, 1), (3, 1), (4, 1), (5, 3), (6, 1), (7, 1), (8, 1), (9, 1), (10, 1), (11, 1), (12, 2), (13, 1), (14, 1), (15, 1), (16, 1), (17, 1), (18, 1), (19, 1), (20, 1), (21, 1), (22, 1), (23, 1), (24, 1), (25, 1), (26, 1), (27, 1), (28, 1)]]\n"
     ]
    }
   ],
   "source": [
    "#bagofword\n",
    "\n",
    "g_bow =[g_dict1.doc2bow(token, allow_update = True) for token in tokens1]\n",
    "print(\"Bag of Words : \", g_bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary : \n",
      "[['be', 1], ['better', 1], ['but', 1], ['can', 1], ['excellent', 1], ['food', 1], ['is', 1], ['service', 1], ['the', 2]]\n",
      "[['food', 1], ['is', 1], ['service', 1], ['the', 2], ['always', 1], ['and', 1], ['delicious', 1], ['loved', 1]]\n",
      "[['food', 1], ['service', 1], ['the', 2], ['and', 1], ['mediocre', 1], ['terrible', 1], ['was', 2]]\n",
      "TF-IDF Vector:\n",
      "[['be', 0.44], ['better', 0.44], ['but', 0.44], ['can', 0.44], ['excellent', 0.44], ['is', 0.16]]\n",
      "[['is', 0.2], ['always', 0.55], ['and', 0.2], ['delicious', 0.55], ['loved', 0.55]]\n",
      "[['and', 0.15], ['mediocre', 0.4], ['terrible', 0.4], ['was', 0.81]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from gensim import corpora, models\n",
    "from gensim.utils import simple_preprocess\n",
    "\n",
    "# Sample text data\n",
    "text = [\"The food is excellent but the service can be better\",\n",
    "        \"The food is always delicious and loved the service\",\n",
    "        \"The food was mediocre and the service was terrible\"]\n",
    "\n",
    "# Create a dictionary and a bag-of-words representation\n",
    "g_dict = corpora.Dictionary([simple_preprocess(line) for line in text])\n",
    "g_bow = [g_dict.doc2bow(simple_preprocess(line)) for line in text]\n",
    "\n",
    "print(\"Dictionary : \")\n",
    "for item in g_bow:\n",
    "    print([[g_dict[id], freq] for id, freq in item])\n",
    "\n",
    "# Create TF-IDF model\n",
    "g_tfidf = models.TfidfModel(g_bow)  # Removed smartirs parameter\n",
    "\n",
    "print(\"TF-IDF Vector:\")\n",
    "for item in g_tfidf[g_bow]:\n",
    "    print([[g_dict[id], np.around(freq, decimals=2)] for id, freq in item])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[==================================================] 100.0% 31.6/31.6MB downloaded\n"
     ]
    }
   ],
   "source": [
    "# wordtovec\n",
    "# from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "words = [d for d in dataset]\n",
    "\n",
    "data1 = words[:1000]\n",
    "w2v_model = Word2Vec(data1, min_count = 0, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models.word2vec import Word2Vec\n",
    "from multiprocessing import cpu_count\n",
    "import gensim.downloader as api\n",
    "\n",
    "dataset = api.load(\"text8\")\n",
    "words = [d for d in dataset]\n",
    "\n",
    "data1 = words[:1000]\n",
    "w2v_model = Word2Vec(data1, min_count = 0, workers=cpu_count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-0.1325466   0.5895778   0.8168837  -0.9248781  -0.22983922 -2.560967\n",
      "  1.3441237   1.5074506   1.5983498  -0.8726959   0.68120265 -1.3661321\n",
      " -0.30371135 -0.4820963  -1.461559   -1.139908   -1.2023104  -1.7519537\n",
      "  2.1427214   1.8094239  -0.923564   -2.6763513  -0.7305989   0.08843581\n",
      "  0.49080527 -2.6083324   3.0280225  -0.9297976   0.5473551   0.78580195\n",
      "  2.017452   -1.0954666   0.4853348  -1.6541517   2.225511   -1.5373356\n",
      "  0.2935346  -0.98239374  1.746962   -2.549666    0.22477241 -1.7043377\n",
      " -1.5138681  -0.5432757  -0.36464897  2.9588993   1.7184076   0.43210918\n",
      " -0.5306053   1.0409236   0.53128475  2.3765965   1.5016122   0.96709746\n",
      "  0.4565198   0.48460463  1.3484075   1.6348147  -2.8479857   1.5030092\n",
      " -0.91517186  1.4320694   0.30033562 -1.2576157  -0.17001116 -0.2486001\n",
      "  1.5167041   1.1957349  -0.3801606   1.8649734  -1.1114717  -1.5237457\n",
      "  1.9586384   1.4528879   0.12418069  1.1242758  -1.2516611   1.6050612\n",
      "  0.16379763  0.04932736  0.68763435 -1.431368    1.2257273  -1.0988573\n",
      " -2.0639484   0.6540851  -0.87305355 -2.057782    0.71850014  1.9790511\n",
      "  1.3063792  -1.5797024   0.14503214 -1.6337659   1.1167994  -1.0401059\n",
      "  0.95379025  0.1952056  -1.3241891   1.9098324 ]\n"
     ]
    }
   ],
   "source": [
    "print(w2v_model.wv['social'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.8393743e+00  1.7876114e+00  5.1164043e-01 -4.6658838e-01\n",
      "  5.8159578e-01 -2.3394954e+00  6.6394061e-01  1.0096829e+00\n",
      " -7.2539604e-01 -1.1951537e+00 -1.5107878e+00 -1.3464013e+00\n",
      " -5.8901185e-01  1.4630330e+00 -1.1333638e+00 -1.4553108e+00\n",
      " -1.1051940e+00 -1.6168203e+00  7.1066660e-01  1.1293647e+00\n",
      "  1.6631817e+00 -1.0479978e+00  3.6292842e-01  5.8696473e-01\n",
      "  9.3067497e-02  9.1374069e-01 -6.5428174e-01  1.1323510e+00\n",
      "  7.0410502e-01 -2.4836771e+00  7.8024381e-01  4.8706117e-01\n",
      " -1.4796766e+00 -1.2942270e+00  9.0216404e-01 -4.6813533e-01\n",
      "  4.7528997e-01 -2.4909945e-01  1.6898478e+00  5.2272599e-02\n",
      " -1.7647772e-01  3.3217123e-01  9.0616190e-01 -7.7822053e-01\n",
      " -5.2487433e-01  1.3902664e+00  4.4226259e-01 -2.8052804e-01\n",
      " -1.7144046e+00 -1.1842011e+00  5.2936333e-01  1.5565002e+00\n",
      " -1.8337011e+00  1.3257382e+00  2.6121816e-01  2.4205768e+00\n",
      "  3.3008184e+00  1.3195591e-01  6.9817388e-01  1.5089024e+00\n",
      " -3.3741373e-01  4.9837548e-01 -1.3774464e+00 -2.6860971e+00\n",
      " -2.0338096e-01 -1.9740363e+00  1.2159714e+00 -7.9668254e-01\n",
      "  4.8504141e-01  5.2985615e-01 -2.3837183e+00 -2.2935388e+00\n",
      "  6.0934365e-01  3.6784929e-01  1.4700459e+00  1.6222535e+00\n",
      " -1.6783898e+00  1.0674040e+00 -1.8821824e+00  2.8379741e-03\n",
      " -1.1769751e+00  1.1366506e+00  1.3588121e+00  2.4849953e-02\n",
      " -2.7474606e+00  8.2332343e-01  1.9767535e+00 -1.4129846e+00\n",
      " -1.7810669e+00 -1.1057851e+00  2.0498097e-01 -4.9439698e-01\n",
      " -6.9176257e-01 -1.8069187e+00  2.4597526e+00 -7.3624051e-01\n",
      " -2.2035675e-01 -3.0678220e+00 -5.1122785e-01 -7.1465570e-01]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Combine original and new data\n",
    "data2 = words[1000:]\n",
    "\n",
    "# Reinitialize and retrain the Word2Vec model with the combined data\n",
    "w2v_model = Word2Vec(data2, vector_size=100, window=5, min_count=1, workers=4)\n",
    "\n",
    "# Access the word vector for the word 'social'\n",
    "word_vector = w2v_model.wv['social']\n",
    "print(word_vector)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.1782252  -1.9462788  -0.88613033 -1.8563372   0.21775442  0.4494338\n",
      "  0.6153668   0.71092194 -1.421506    0.03973827 -0.15387054  2.5989013\n",
      " -2.678018   -0.5616163   3.0806756   1.622365   -0.4769562  -1.5271864\n",
      " -1.4485598  -2.0827227   0.9139828  -3.3948226  -4.5022373  -0.5102006\n",
      "  0.27083683 -1.8925384   3.396334    0.12853554  2.6961741  -1.4811388\n",
      "  2.512643   -1.1933365  -0.45729724  0.785515    2.17609    -2.1605625\n",
      " -0.46608528  1.0052353   2.5228107   1.2262905   0.44548503  0.3791743\n",
      " -1.3272699   1.607254    0.6369766   2.5514555   1.946156    0.40694842\n",
      " -0.20645112  0.46486983  0.1688158  -1.7176708   1.7051053   1.6066723\n",
      "  3.3949668  -0.14284109  1.9688152   0.5332622   2.174993   -2.2266629\n",
      " -1.367477    2.7208707  -0.3154919  -3.8008866  -1.3741033   2.4302692\n",
      "  0.71736354 -0.05528111 -0.9655665   4.2917056  -1.8391315  -0.281788\n",
      "  0.87163484  0.9273873  -0.8181277   1.1350526   0.985348   -0.93440354\n",
      " -0.73087233 -3.3110116  -0.07789669 -0.40077257 -0.33987856  0.59601855\n",
      "  0.02004418  1.2483046   0.67114097 -0.65702873 -0.05620942 -0.9035247\n",
      " -0.33588055  0.82726884 -1.42052    -1.9510518  -0.34916756  0.12675515\n",
      " -0.4741076  -0.5011092  -2.3648274   1.5471318 ]\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Assuming 'words' is your corpus data\n",
    "data = words  # Use your existing data variable\n",
    "\n",
    "# Initialize and train the Word2Vec model\n",
    "w2v_model = Word2Vec(data, vector_size=100, window=5, min_count=1, workers=4, negative=10)\n",
    "\n",
    "# If you want to update the model with additional data\n",
    "data2 = words[1000:]\n",
    "w2v_model.build_vocab(data2, update=True)\n",
    "w2v_model.train(data2, total_examples=w2v_model.corpus_count, epochs=w2v_model.epochs)\n",
    "\n",
    "# Access the word vector for the word 'social'\n",
    "word_vector = w2v_model.wv['social']\n",
    "print(word_vector)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
